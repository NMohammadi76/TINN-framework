{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Theoretical-Informed Neural Network (TINN) for Parameter Estimation in DDM model\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import scipy.optimize\n",
        "import pandas as pd\n",
        "from scipy.stats import gaussian_kde\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# =============================================================================\n",
        "# Configuration\n",
        "# =============================================================================\n",
        "\n",
        "# Simulation parameters\n",
        "NUM_SIMULATIONS = 2000\n",
        "TIME_STEPS = 20000\n",
        "DT = 0.01\n",
        "NON_DECISION_TIME = 0.3\n",
        "\n",
        "# True parameters for DDM\n",
        "DRIFT_RATE = 0.5\n",
        "BOUNDARY_SEPARATION = 2.0\n",
        "STARTING_POINT = 0.0\n",
        "NOISE_STD = 1.0\n",
        "\n",
        "# TINN training parameters\n",
        "NUM_EPOCHS = 50000\n",
        "NUM_HIDDEN_LAYERS = 4\n",
        "NUM_NEURONS_PER_LAYER = 30\n",
        "THRESHOLD = 2.0\n",
        "\n",
        "# =============================================================================\n",
        "# DDM Simulation\n",
        "# =============================================================================\n",
        "\n",
        "def simulate_ddm_rk4(num_simulations, time_steps, drift_rate, boundary_separation, starting_point, noise_std, dt):\n",
        "    decision_times1 = []\n",
        "    decision_times2 = []\n",
        "    noise = np.random.normal(0, noise_std, (num_simulations, time_steps))\n",
        "\n",
        "    for i in range(num_simulations):\n",
        "        decision_variable = starting_point\n",
        "        for t in range(time_steps):\n",
        "            k1 = drift_rate * dt + noise[i, t] * np.sqrt(dt)\n",
        "            k2 = drift_rate * dt + noise[i, t] * np.sqrt(dt)\n",
        "            k3 = drift_rate * dt + noise[i, t] * np.sqrt(dt)\n",
        "            k4 = drift_rate * dt + noise[i, t] * np.sqrt(dt)\n",
        "            decision_variable += (k1 + 2*k2 + 2*k3 + k4) / 6\n",
        "\n",
        "            if decision_variable >= boundary_separation:\n",
        "                decision_times1.append(t * dt)\n",
        "                break\n",
        "            elif decision_variable <= -boundary_separation:\n",
        "                decision_times2.append(t * dt)\n",
        "                break\n",
        "    return np.array(decision_times1), np.array(decision_times2)\n",
        "\n",
        "# =============================================================================\n",
        "# TINN Architecture\n",
        "# =============================================================================\n",
        "\n",
        "class TINN_NeuralNet(tf.keras.Model):\n",
        "    def __init__(self, lb, ub, output_dim=1, num_hidden_layers=5, num_neurons_per_layer=50, activation='tanh', kernel_initializer='glorot_normal', **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.output_dim = output_dim\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer, activation=tf.keras.activations.get(activation), kernel_initializer=kernel_initializer) for _ in range(self.num_hidden_layers-1)]\n",
        "        self.hidden1 = tf.keras.layers.Dense(num_neurons_per_layer, activation='softplus')\n",
        "        self.out = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, X):\n",
        "        Z = X\n",
        "        for i in range(self.num_hidden_layers-1):\n",
        "            Z = self.hidden[i](Z)\n",
        "        return self.out(Z)\n",
        "\n",
        "class TINNIdentificationNet(TINN_NeuralNet):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.lambd = self.add_weight(name=\"lambd_raw\", initializer=\"ones\", trainable=True, dtype=tf.float32)\n",
        "        self.sig = self.add_weight(name=\"sig\", initializer=\"ones\", trainable=True, dtype=tf.float32)\n",
        "        self.tt0 = self.add_weight(name=\"tt0\", initializer=\"ones\", trainable=True, dtype=tf.float32)\n",
        "        self.lambd_list = []\n",
        "        self.sig_list = []\n",
        "        self.tt0_list = []\n",
        "\n",
        "# =============================================================================\n",
        "# TINN Solver\n",
        "# =============================================================================\n",
        "\n",
        "class TINNSolver():\n",
        "    def __init__(self, model, X_r):\n",
        "        self.model = model\n",
        "        self.t = X_r[:,0:1]\n",
        "        self.x = X_r[:,1:2]\n",
        "        self.hist = []\n",
        "        self.iter = 0\n",
        "\n",
        "    def get_r(self):\n",
        "        with tf.GradientTape(persistent=False) as tape:\n",
        "            tape.watch(self.t)\n",
        "            tape.watch(self.x)\n",
        "            u = self.model(tf.stack([self.t[:,0], self.x[:,0]], axis=1))\n",
        "            u_x = tape.gradient(u, self.x)\n",
        "        u_t = tape.gradient(u, self.t)\n",
        "        u_xx = tape.gradient(u_x, self.x)\n",
        "        del tape\n",
        "        return self.fun_r(self.t, self.x, u, u_t, u_x, u_xx)\n",
        "\n",
        "    def loss_fn(self, X, xmax, rt1, rt2, u):\n",
        "        r = self.get_r()\n",
        "        phi_r = tf.reduce_mean(tf.square(r))\n",
        "        loss_r = phi_r\n",
        "\n",
        "        u_pred = self.model(X[0])\n",
        "        loss_0 = tf.reduce_mean(tf.square(u[0] - u_pred))\n",
        "\n",
        "        u_pred = self.model(X[1])\n",
        "        loss_b = tf.reduce_mean(tf.square(u[1] - u_pred))\n",
        "\n",
        "        tspace1 = np.sort(rt1)\n",
        "        tspace2 = np.sort(rt2)\n",
        "        tspace_tf1 = tf.constant(tspace1.reshape((len(rt1),1)), 'float32')\n",
        "        minRT1 = min(rt1)\n",
        "        minRT2 = min(rt2)\n",
        "        self.minRT = min(minRT1, minRT2)\n",
        "        t1 = tspace_tf1 - tf.math.sigmoid(self.model.tt0)*self.minRT\n",
        "\n",
        "        xspace = np.ones((len(tspace1),1))*xmax\n",
        "        xtf = tf.constant(xspace, 'float32')\n",
        "        X_bound = tf.concat([t1, xtf], 1)\n",
        "        p_i = self.model(X_bound)\n",
        "\n",
        "        xspace1 = np.ones((len(tspace1),1))*(xmax-0.02)\n",
        "        xtf1 = tf.constant(xspace1, 'float32')\n",
        "        X1 = tf.concat([t1, xtf1], 1)\n",
        "        p_ii = self.model(X1)\n",
        "\n",
        "        xspace2 = np.ones((len(tspace1),1))*(xmax-0.04)\n",
        "        xtf2 = tf.constant(xspace2, 'float32')\n",
        "        X2 = tf.concat([t1, xtf2], 1)\n",
        "        p_iii = self.model(X2)\n",
        "        p_x = (3*p_i - 4*p_ii + p_iii)/(2*0.02)\n",
        "\n",
        "        J1 = tf.nn.softplus(self.model.lambd)* p_i - 0.5*(self.model.sig)**2*p_x\n",
        "        kde1 = gaussian_kde(tspace1)\n",
        "        p_kde1 = len(tspace1)*kde1(tspace1)[:, np.newaxis]/(len(tspace2)+len(tspace1))\n",
        "        p_kde_tensor1 = tf.convert_to_tensor(p_kde1, dtype=tf.float32)\n",
        "        KDE_loss1 = tf.reduce_mean(tf.square(J1 - p_kde_tensor1))\n",
        "\n",
        "        tspace_tf2 = tf.constant(tspace2.reshape((len(rt2),1)), 'float32')\n",
        "        t2 = tspace_tf2 - tf.math.sigmoid(self.model.tt0)*self.minRT\n",
        "\n",
        "        xspace20 = np.ones((len(tspace2),1))*(-xmax)\n",
        "        xtf20 = tf.constant(xspace20, 'float32')\n",
        "        X20 = tf.concat([t2, xtf20], 1)\n",
        "        p_i2 = self.model(X20)\n",
        "\n",
        "        xspace21 = np.ones((len(tspace2),1))*(-xmax+0.02)\n",
        "        xtf21 = tf.constant(xspace21, 'float32')\n",
        "        X21 = tf.concat([t2, xtf21], 1)\n",
        "        p_ii2 = self.model(X21)\n",
        "\n",
        "        xspace22 = np.ones((len(tspace2),1))*(-xmax+0.04)\n",
        "        xtf22 = tf.constant(xspace22, 'float32')\n",
        "        X22 = tf.concat([t2, xtf22], 1)\n",
        "        p_iii2 = self.model(X22)\n",
        "        p_x2 = (-3*p_i2 + 4*p_ii2 - p_iii2)/(2*0.02)\n",
        "\n",
        "        J2 = tf.nn.softplus(self.model.lambd)* p_i2 - 0.5*(self.model.sig)**2*p_x2\n",
        "        kde2 = gaussian_kde(tspace2)\n",
        "        p_kde2 = -len(tspace2)*kde2(tspace2)[:, np.newaxis]/(len(tspace2)+len(tspace1))\n",
        "        p_kde_tensor2 = tf.convert_to_tensor(p_kde2, dtype=tf.float32)\n",
        "        KDE_loss2 = tf.reduce_mean(tf.square(J2 - p_kde_tensor2))\n",
        "\n",
        "        return 100*(loss_r + loss_0 + loss_b) + KDE_loss1 + KDE_loss2\n",
        "\n",
        "    def get_grad(self, X, xmax, rt1, rt2, u):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.loss_fn(X, xmax, rt1, rt2, u)\n",
        "        g = tape.gradient(loss, self.model.trainable_variables)\n",
        "        del tape\n",
        "        return loss, g\n",
        "\n",
        "    def solve_with_TFoptimizer(self, optimizer, X, xmax, rt1, rt2, u, N=1001):\n",
        "        @tf.function\n",
        "        def train_step():\n",
        "            loss, grad_theta = self.get_grad(X, xmax, rt1, rt2, u)\n",
        "            optimizer.apply_gradients(zip(grad_theta, self.model.trainable_variables))\n",
        "            return loss\n",
        "\n",
        "        for i in range(N):\n",
        "            loss = train_step()\n",
        "            self.current_loss = loss.numpy()\n",
        "            self.callback()\n",
        "\n",
        "    def callback(self, xr=None):\n",
        "        if self.iter % 50 == 0:\n",
        "            print('It {:05d}: loss = {:10.8e}'.format(self.iter, self.current_loss))\n",
        "        self.hist.append(self.current_loss)\n",
        "        self.iter += 1\n",
        "\n",
        "# =============================================================================\n",
        "# Fokker-Planck TINN Solver\n",
        "# =============================================================================\n",
        "\n",
        "class F_P_TINNSolver(TINNSolver):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def get_r(self):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            tape.watch(self.t)\n",
        "            tape.watch(self.x)\n",
        "            u = self.model(tf.stack([self.t[:,0], self.x[:,0]], axis=1))\n",
        "            u_x = tape.gradient(u, self.x)\n",
        "        u_t = tape.gradient(u, self.t)\n",
        "        u_xx = tape.gradient(u_x, self.x)\n",
        "        del tape\n",
        "        return self.fun_r(self.t, self.x, u, u_t, u_x, u_xx)\n",
        "\n",
        "class F_P_TINNIdentification(F_P_TINNSolver):\n",
        "    def fun_r(self, t, x, u, u_t, u_x, u_xx):\n",
        "        return u_t + tf.nn.softplus(self.model.lambd)*u_x - 0.5*(self.model.sig)**2*u_xx\n",
        "\n",
        "    def callback(self, xr=None):\n",
        "        mu = tf.nn.softplus(self.model.lambd)\n",
        "        sigma = self.model.sig\n",
        "        lambd = mu.numpy()\n",
        "        sig = sigma.numpy()\n",
        "        tt0 = (tf.math.sigmoid(self.model.tt0)*self.minRT).numpy()\n",
        "        self.model.lambd_list.append(lambd)\n",
        "        self.model.sig_list.append(sig)\n",
        "        self.model.tt0_list.append(tt0)\n",
        "        if self.iter % 50 == 0:\n",
        "            print('It {:05d}: loss = {:10.8e} drift = {:10.8e} sigma = {:10.8e} t0 = {:10.8e}'.format(self.iter, self.current_loss, lambd, sig, tt0-0.1))\n",
        "        self.hist.append(self.current_loss)\n",
        "        self.iter += 1\n",
        "\n",
        "    def lambdaf(self):\n",
        "        mu = tf.nn.softplus(self.model.lambd)\n",
        "        return mu.numpy()\n",
        "\n",
        "    def sigmaf(self):\n",
        "        return self.model.sig.numpy()\n",
        "\n",
        "    def tt0f(self):\n",
        "        return (tf.math.sigmoid(self.model.tt0)*self.minRT).numpy()\n",
        "\n",
        "# =============================================================================\n",
        "# Main Execution\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"TINN Parameter Estimation for Drift Diffusion Model\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Generate synthetic data\n",
        "    print(\"Generating synthetic DDM data...\")\n",
        "    decision_times1, decision_times2 = simulate_ddm_rk4(NUM_SIMULATIONS, TIME_STEPS, DRIFT_RATE, BOUNDARY_SEPARATION, STARTING_POINT, NOISE_STD, DT)\n",
        "    rt1 = np.sort(decision_times1) + NON_DECISION_TIME\n",
        "    rt2 = np.sort(decision_times2) + NON_DECISION_TIME\n",
        "    print(f\"Generated {len(rt1)} upper and {len(rt2)} lower boundary crossings\")\n",
        "\n",
        "    # Prepare domain and training data\n",
        "    xmax = THRESHOLD\n",
        "    xmin = -xmax\n",
        "    maxrt1 = max(rt1)\n",
        "    maxrt2 = max(rt2)\n",
        "    lb = tf.constant([0, xmin], dtype='float32')\n",
        "    ub = tf.constant([max(maxrt1, maxrt2), xmax], dtype='float32')\n",
        "\n",
        "    # Collocation points\n",
        "    N = 100\n",
        "    tspace = np.linspace(lb[0], ub[0], N + 1)\n",
        "    xspace = np.linspace(lb[1], ub[1], N + 1)\n",
        "    T, X = np.meshgrid(tspace, xspace)\n",
        "    Xgrid = np.vstack([T.flatten(), X.flatten()]).T\n",
        "    Xgrid = tf.constant(Xgrid, 'float32')\n",
        "\n",
        "    # Boundary conditions\n",
        "    def ddf(x, delta, x0):\n",
        "        return 1/(2*np.sqrt(np.pi*delta))*tf.math.exp(-((x-x0)**2)/(4*delta))\n",
        "\n",
        "    def fun_u_0(x):\n",
        "        return ddf(x, 7.8e-2, 0)\n",
        "\n",
        "    N_b = 500\n",
        "    t_b = tf.random.uniform((N_b,1), lb[0], ub[0], dtype='float32')\n",
        "    x_b = lb[1] + (ub[1] - lb[1]) * tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype='float32')\n",
        "    X_b = tf.concat([t_b, x_b], axis=1)\n",
        "    u_b = tf.zeros(tf.shape(x_b), 'float32')\n",
        "\n",
        "    N_0 = 50\n",
        "    t_0 = tf.ones((N_0,1), dtype='float32')*lb[0]\n",
        "    x_0 = np.linspace(lb[1], ub[1], N_0-1, dtype='float32')\n",
        "    x_0 = np.asarray(list(x_0) + [0.0])\n",
        "    x_0 = np.sort(x_0)\n",
        "    x_0 = tf.convert_to_tensor(x_0, dtype='float32')\n",
        "    x_0 = tf.reshape(x_0, [N_0,1])\n",
        "    u_0 = fun_u_0(x_0)\n",
        "    X_0 = tf.concat([t_0, x_0], axis=1)\n",
        "\n",
        "    X_param = [X_0, X_b]\n",
        "    u_param = [u_0, u_b]\n",
        "\n",
        "    # Initialize and train TINN\n",
        "    model = TINNIdentificationNet(lb, ub, num_hidden_layers=NUM_HIDDEN_LAYERS, num_neurons_per_layer=NUM_NEURONS_PER_LAYER, activation='tanh', kernel_initializer='glorot_normal')\n",
        "\n",
        "    f_p_Identification = F_P_TINNIdentification(model, Xgrid)\n",
        "\n",
        "    lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,15000], [0.01,0.001,0.0005])\n",
        "    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    print(\"Starting TINN training...\")\n",
        "    t0 = time()\n",
        "    f_p_Identification.solve_with_TFoptimizer(optim, X_param, xmax, rt1, rt2, u_param, N=NUM_EPOCHS)\n",
        "    print('\\nComputation time: {} seconds'.format(time()-t0))\n",
        "\n",
        "    # Results\n",
        "    print(\"\\nTrue parameters: Drift = {:.4f}, Noise = {:.4f}, Non-decision = {:.4f}\".format(DRIFT_RATE, NOISE_STD, NON_DECISION_TIME))\n",
        "    print(\"Estimated parameters: Drift = {:.4f}, Noise = {:.4f}, Non-decision = {:.4f}\".format(f_p_Identification.lambdaf(), f_p_Identification.sigmaf(), f_p_Identification.tt0f()-0.1))\n",
        "\n",
        "    # Visualization\n",
        "    plot_results(model, f_p_Identification, rt1, rt2, lb, ub, DRIFT_RATE, BOUNDARY_SEPARATION)\n",
        "\n",
        "def plot_results(model, solver, rt1, rt2, lb, ub, drift_rate, boundary_separation):\n",
        "    # Analytical solution\n",
        "    def f(v, a, t):\n",
        "        return (a/np.sqrt(2*np.pi*t**3)*np.exp(-(v*t-a)**2/(2*t)))\n",
        "\n",
        "    # Upper boundary\n",
        "    M = 500\n",
        "    tspace = np.linspace(0, ub[0], M + 1)\n",
        "    xspace = np.ones((M+1))*ub[1]\n",
        "    X = np.zeros((M+1,2))\n",
        "    X[:,0] = tspace\n",
        "    X[:,1] = xspace\n",
        "    X = tf.constant(X, 'float32')\n",
        "    p_i = model(X)\n",
        "\n",
        "    xspace1 = np.ones((M+1))*(ub[1]-0.02)\n",
        "    X1 = np.zeros((M+1,2))\n",
        "    X1[:,0] = tspace\n",
        "    X1[:,1] = xspace1\n",
        "    X1 = tf.constant(X1, 'float32')\n",
        "    p_ii = model(X1)\n",
        "\n",
        "    xspace2 = np.ones((M+1))*(ub[1]-0.04)\n",
        "    X2 = np.zeros((M+1,2))\n",
        "    X2[:,0] = tspace\n",
        "    X2[:,1] = xspace2\n",
        "    X2 = tf.constant(X2, 'float32')\n",
        "    p_iii = model(X2)\n",
        "    p_x = (3*p_i - 4*p_ii + p_iii)/(2*0.02)\n",
        "    J1 = solver.lambdaf()* p_i - 0.5*solver.sigmaf()**2*p_x\n",
        "\n",
        "    F1 = [f(drift_rate, boundary_separation, rt) for rt in np.sort(tspace)]\n",
        "    kde1 = gaussian_kde(rt1)\n",
        "\n",
        "    font = {'family': 'serif', 'color': 'black', 'weight': 'normal', 'size': 16}\n",
        "\n",
        "    plt.figure(figsize=(12, 8), dpi=90)\n",
        "    plt.plot(tspace + 0.07, J1, label='TINN Approximation', color='black')\n",
        "    plt.plot(tspace, np.asarray(F1), label='Analytical Solution', color='black', linestyle='--')\n",
        "    plt.plot(rt1 - 0.3, len(rt1)*kde1(rt1)[:, np.newaxis]/(len(rt1)+len(rt2)), label='Empirical Data', color='red', linestyle='-.')\n",
        "    plt.xlabel('Time', fontdict=font)\n",
        "    plt.ylabel('First Passage Density', fontdict=font)\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig(\"tinn_upper_boundary.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nAnalysis completed successfully!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "LtLC916YAfuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VL7wPssWAjNn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}