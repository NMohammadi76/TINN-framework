{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwNg6qDM-dw7"
      },
      "outputs": [],
      "source": [
        "#import pyddm\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from time import time\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import scipy.optimize\n",
        "import pandas as pd\n",
        "from scipy.stats import gaussian_kde\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "\n",
        "\n",
        "def simulation_ddm(num_simulations, time_steps,drift_rate, boundary_separation, starting_point, noise_std, dt):\n",
        "  # Arrays to store results\n",
        "  decision_times1 = []\n",
        "  decision_times2 = []\n",
        "  no_decision = []\n",
        "\n",
        "  for i in range(num_simulations):\n",
        "      # Initialize decision variable\n",
        "      decision_variable = starting_point\n",
        "      for t in range(time_steps):\n",
        "          # Update decision variable with drift and noise\n",
        "          noise = np.random.normal(0, noise_std)\n",
        "          decision_variable += drift_rate * dt + noise * np.sqrt(dt)\n",
        "\n",
        "          # Check if decision boundary is crossed\n",
        "          if decision_variable >= boundary_separation:\n",
        "              decision_times1.append(t * dt)\n",
        "              break\n",
        "          elif decision_variable <= -boundary_separation:\n",
        "              decision_times2.append(t * dt)\n",
        "              break\n",
        "      else:\n",
        "          # If no decision is made within time_steps, record maximum time\n",
        "          no_decision.append(time_steps * dt)\n",
        "  return decision_times1,decision_times2\n",
        "\n",
        "\n",
        "def simulation_ddm_rk4(num_simulations, time_steps, drift_rate, boundary_separation, starting_point, noise_std, dt):\n",
        "    # Arrays to store results\n",
        "    decision_times1 = []\n",
        "    decision_times2 = []\n",
        "    no_decision = []\n",
        "\n",
        "    # Generate noise once for efficiency\n",
        "    noise = np.random.normal(0, noise_std, (num_simulations, time_steps))\n",
        "\n",
        "    for i in range(num_simulations):\n",
        "        # Initialize decision variable\n",
        "        decision_variable = starting_point\n",
        "        for t in range(time_steps):\n",
        "            # RK4 method: compute four slopes (k1, k2, k3, k4)\n",
        "            k1 = drift_rate * dt + noise[i, t] * np.sqrt(dt)\n",
        "            k2 = drift_rate * dt + noise[i, t] * np.sqrt(dt)\n",
        "            k3 = drift_rate * dt + noise[i, t] * np.sqrt(dt)\n",
        "            k4 = drift_rate * dt + noise[i, t] * np.sqrt(dt)\n",
        "\n",
        "            # Update decision variable using the RK4 formula\n",
        "            decision_variable += (k1 + 2 * k2 + 2 * k3 + k4) / 6\n",
        "\n",
        "            # Check if decision boundary is crossed\n",
        "            if decision_variable >= boundary_separation:\n",
        "                decision_times1.append(t * dt)  # Time of first boundary crossing\n",
        "                break\n",
        "            elif decision_variable <= -boundary_separation:\n",
        "                decision_times2.append(t * dt)  # Time of second boundary crossing\n",
        "                break\n",
        "        else:\n",
        "            # If no decision is made within time_steps, record maximum time\n",
        "            no_decision.append(time_steps * dt)\n",
        "\n",
        "    # Return results\n",
        "    return np.array(decision_times1), np.array(decision_times2)\n",
        "\n",
        "\n",
        "\n",
        "# A sample\n",
        "# Simulation parameters\n",
        "num_simulations = 4000  # Number of simulated decision processes\n",
        "time_steps = 20000       # Number of time steps per simulation\n",
        "dt = 0.01               # Time step size\n",
        "drift_rate = .5       # Drift rate (v)\n",
        "boundary_separation = 2  # Boundary separation (a)\n",
        "starting_point = 0.0     # Starting point (z)\n",
        "noise_std = 1.     # Standard deviation of noise (σ)\n",
        "\n",
        "decision_times1,decision_times2 = simulation_ddm_rk4(num_simulations, time_steps,drift_rate, boundary_separation, starting_point, noise_std, dt)\n",
        "rt1 = np.sort(decision_times1)\n",
        "rt2 = np.sort(decision_times2)\n",
        "\n",
        "kde1 = gaussian_kde(rt1)\n",
        "plt.plot(rt1,len(rt1)*kde1(rt1)[:, np.newaxis]/(len(rt1)+len(rt2)))\n",
        "\n",
        "kde2 = gaussian_kde(rt2)\n",
        "plt.plot(rt2,-len(rt2)*kde2(rt2)[:, np.newaxis]/(len(rt1)+len(rt2)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define model architecture\n",
        "class PINN_NeuralNet(tf.keras.Model):\n",
        "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
        "\n",
        "    def __init__(self, lb, ub,\n",
        "            output_dim=1,\n",
        "            num_hidden_layers=5,\n",
        "            num_neurons_per_layer=50,\n",
        "            activation='tanh',\n",
        "            kernel_initializer='glorot_normal',\n",
        "            **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.output_dim = output_dim\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        ## Define NN architecture\n",
        "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
        "                             activation=tf.keras.activations.get(activation),\n",
        "                             kernel_initializer=kernel_initializer)\n",
        "                           for _ in range(self.num_hidden_layers-1)]\n",
        "        self.hidden1 = tf.keras.layers.Dense(num_neurons_per_layer,activation = 'softplus')\n",
        "        self.out = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, X):\n",
        "        \"\"\"Forward-pass through neural network.\"\"\"\n",
        "        Z = X\n",
        "        for i in range(self.num_hidden_layers-1):\n",
        "            Z = self.hidden[i](Z)\n",
        "        #Z = self.hidden1(Z)\n",
        "        return self.out(Z)\n",
        "\n",
        "class PINNIdentificationNet(PINN_NeuralNet):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        # Call init of base class\n",
        "        super().__init__(*args,**kwargs)\n",
        "\n",
        "        # Initialize variable for lambda\n",
        "        #self.lambd = tf.Variable(0.0, trainable=True, dtype='float32')\n",
        "        #self.sig = tf.Variable(2.0, trainable=True, dtype='float32')\n",
        "        #self.tt0 = tf.Variable(0.0, trainable=True, dtype='float32')\n",
        "        self.lambd = self.add_weight(\n",
        "            name=\"lambd_raw\", initializer=\"ones\",trainable=True, dtype=tf.float32)\n",
        "        self.sig = self.add_weight(name=\"sig\", initializer=\"ones\" ,trainable=True, dtype=tf.float32)\n",
        "        self.tt0 = self.add_weight(name=\"tt0\",initializer=\"ones\",trainable=True, dtype=tf.float32)\n",
        "\n",
        "        self.lambd_list = []\n",
        "        self.sig_list = []\n",
        "        self.tt0_list = []\n",
        "\n",
        "\n",
        "class PINNSolver():\n",
        "    def __init__(self, model, X_r):\n",
        "        self.model = model\n",
        "\n",
        "        # Store collocation points\n",
        "        self.t = X_r[:,0:1]\n",
        "        self.x = X_r[:,1:2]\n",
        "\n",
        "        # Initialize history of losses and global iteration counter\n",
        "        self.hist = []\n",
        "        self.iter = 0\n",
        "\n",
        "    def get_r(self):\n",
        "\n",
        "        with tf.GradientTape(persistent=False) as tape:\n",
        "            # Watch variables representing t and x during this GradientTape\n",
        "            tape.watch(self.t)\n",
        "            tape.watch(self.x)\n",
        "\n",
        "            # Compute current values u(t,x)\n",
        "            u = self.model(tf.stack([self.t[:,0], self.x[:,0]], axis=1))\n",
        "\n",
        "            u_x = tape.gradient(u, self.x)\n",
        "\n",
        "        u_t = tape.gradient(u, self.t)\n",
        "        u_xx = tape.gradient(u_x, self.x)\n",
        "\n",
        "        del tape\n",
        "        return self.fun_r(self.t, self.x, u, u_t, u_x, u_xx)\n",
        "\n",
        "    def loss_fn(self, X,xmax,rt1,rt2, u):\n",
        "        # Compute phi_r\n",
        "        r= self.get_r()\n",
        "        phi_r = tf.reduce_mean(tf.square(r))\n",
        "\n",
        "        # Initialize loss\n",
        "        loss_r = phi_r\n",
        "\n",
        "        # Add phi_0 and phi_b to the loss\n",
        "\n",
        "        u_pred = self.model(X[0])\n",
        "        loss_0 = tf.reduce_mean(tf.square(u[0] - u_pred))\n",
        "        #loss_0 = tf.reduce_mean(tf.square(tfp.math.trapz(u_pred, x=X[0][:,1:2])-1))\n",
        "\n",
        "        u_pred = self.model(X[1])\n",
        "        loss_b = tf.reduce_mean(tf.square(u[1] - u_pred))\n",
        "\n",
        "        tspace1 = np.sort(rt1)\n",
        "        tspace2 = np.sort(rt2)\n",
        "        tspace_tf1 = tf.constant(tspace1.reshape((len(rt1),1)),'float32')\n",
        "        minRT1 = min(rt1)\n",
        "        minRT2 = min(rt2)\n",
        "        self.minRT = min(minRT1,minRT2)\n",
        "        t1 = tspace_tf1 - tf.math.sigmoid(self.model.tt0)*self.minRT\n",
        "\n",
        "\n",
        "        xspace = np.ones((len(tspace1),1))*xmax\n",
        "        xtf = tf.constant(xspace,'float32')\n",
        "        X = tf.concat([t1,xtf],1)\n",
        "        p_i =  self.model(X)\n",
        "\n",
        "        xspace1 = np.ones((len(tspace1),1))*(xmax-0.02)\n",
        "        xtf1 = tf.constant(xspace1,'float32')\n",
        "        X1 = tf.concat([t1,xtf1],1)\n",
        "        p_ii =  self.model(X1)\n",
        "\n",
        "        xspace2 = np.ones((len(tspace1),1))*(xmax-0.04)\n",
        "        xtf2 = tf.constant(xspace2,'float32')\n",
        "        X2 = tf.concat([t1,xtf2],1)\n",
        "        p_iii =  self.model(X2)\n",
        "        p_x = (3*p_i-4*p_ii+p_iii)/(2*0.02)\n",
        "\n",
        "        J1 = tf.nn.softplus(self.model.lambd)* p_i -0.5*(self.model.sig)**2*p_x\n",
        "        kde1 = gaussian_kde(tspace1)\n",
        "        p_kde1 = len(tspace1)*kde1(tspace1)[:, np.newaxis]/(len(tspace2)+len(tspace1))\n",
        "        p_kde_tensor1 = tf.convert_to_tensor(p_kde1, dtype=tf.float32)\n",
        "        KDE_loss1 = tf.reduce_mean(tf.square(J1 - p_kde_tensor1))\n",
        "\n",
        "#####################################################3\n",
        "        tspace_tf2 = tf.constant(tspace2.reshape((len(rt2),1)),'float32')\n",
        "\n",
        "        t2 = tspace_tf2 - tf.math.sigmoid(self.model.tt0)*self.minRT\n",
        "\n",
        "\n",
        "        xspace20 = np.ones((len(tspace2),1))*(-xmax)\n",
        "        xtf20 = tf.constant(xspace20,'float32')\n",
        "        X20 = tf.concat([t2,xtf20],1)\n",
        "        p_i2 =  self.model(X20)\n",
        "\n",
        "        xspace21 = np.ones((len(tspace2),1))*(-xmax+0.02)\n",
        "        xtf21 = tf.constant(xspace21,'float32')\n",
        "        X21 = tf.concat([t2,xtf21],1)\n",
        "        p_ii2 =  self.model(X21)\n",
        "\n",
        "        xspace22 = np.ones((len(tspace2),1))*(-xmax+0.04)\n",
        "        xtf22 = tf.constant(xspace22,'float32')\n",
        "        X22 = tf.concat([t2,xtf22],1)\n",
        "        p_iii2 =  self.model(X22)\n",
        "        p_x2 = (-3*p_i2+4*p_ii2 -p_iii2)/(2*0.02)\n",
        "\n",
        "        J2 = tf.nn.softplus(self.model.lambd)* p_i2 -0.5*(self.model.sig)**2*p_x2\n",
        "        kde2 = gaussian_kde(tspace2)\n",
        "        p_kde2 =- len(tspace2)*kde2(tspace2)[:, np.newaxis]/(len(tspace2)+len(tspace1))\n",
        "        p_kde_tensor2 = tf.convert_to_tensor(p_kde2, dtype=tf.float32)\n",
        "        KDE_loss2 = tf.reduce_mean(tf.square(J2 - p_kde_tensor2))\n",
        "\n",
        "        return 100*(loss_r+loss_0+loss_b) + KDE_loss1 + KDE_loss2\n",
        "\n",
        "\n",
        "    def get_grad(self, X,xmax,rt1,rt2, u):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # This tape is for derivatives with\n",
        "            # respect to trainable variables\n",
        "            #tape.watch(self.model.trainable_variables)\n",
        "\n",
        "            loss = self.loss_fn(X,xmax,rt1,rt2, u)\n",
        "\n",
        "        g = tape.gradient(loss, self.model.trainable_variables)\n",
        "        del tape\n",
        "        return loss, g\n",
        "\n",
        "    #def fun_r(self, t, x, u, u_t, u_x, u_xx):\n",
        "        \"\"\"Residual of the PDE\"\"\"\n",
        "     #   return u_t + tf.nn.softplus(self.model.lambd)*u_x - 0.5 *(tf.nn.softplus(self.model.sig))**2* u_xx\n",
        "\n",
        "    def solve_with_TFoptimizer(self, optimizer, X,xmax,rt1,rt2, u, N=1001):\n",
        "        \"\"\"This method performs a gradient descent type optimization.\"\"\"\n",
        "\n",
        "        @tf.function\n",
        "        def train_step():\n",
        "            loss, grad_theta = self.get_grad(X,xmax,rt1,rt2, u)\n",
        "\n",
        "            # Perform gradient descent step\n",
        "            optimizer.apply_gradients(zip(grad_theta, self.model.trainable_variables))\n",
        "            return loss\n",
        "\n",
        "        for i in range(N):\n",
        "\n",
        "            loss = train_step()\n",
        "\n",
        "            self.current_loss = loss.numpy()\n",
        "            self.callback()\n",
        "\n",
        "    def solve_with_ScipyOptimizer(self, X,rt, u, method='L-BFGS-B', **kwargs):\n",
        "        \"\"\"This method provides an interface to solve the learning problem\n",
        "        using a routine from scipy.optimize.minimize.\n",
        "        (Tensorflow 1.xx had an interface implemented, which is not longer\n",
        "        supported in Tensorflow 2.xx.)\n",
        "        Type conversion is necessary since scipy-routines are written in Fortran\n",
        "        which requires 64-bit floats instead of 32-bit floats.\"\"\"\n",
        "\n",
        "        def get_weight_tensor():\n",
        "            \"\"\"Function to return current variables of the model\n",
        "            as 1d tensor as well as corresponding shapes as lists.\"\"\"\n",
        "\n",
        "            weight_list = []\n",
        "            shape_list = []\n",
        "\n",
        "            # Loop over all variables, i.e. weight matrices, bias vectors and unknown parameters\n",
        "            for v in self.model.variables:\n",
        "                shape_list.append(v.shape)\n",
        "                weight_list.extend(v.numpy().flatten())\n",
        "\n",
        "            weight_list = tf.convert_to_tensor(weight_list)\n",
        "            return weight_list, shape_list\n",
        "\n",
        "        x0, shape_list = get_weight_tensor()\n",
        "\n",
        "        def set_weight_tensor(weight_list):\n",
        "            \"\"\"Function which sets list of weights\n",
        "            to variables in the model.\"\"\"\n",
        "            idx = 0\n",
        "            for v in self.model.variables:\n",
        "                vs = v.shape\n",
        "\n",
        "                # Weight matrices\n",
        "                if len(vs) == 2:\n",
        "                    sw = vs[0]*vs[1]\n",
        "                    new_val = tf.reshape(weight_list[idx:idx+sw],(vs[0],vs[1]))\n",
        "                    idx += sw\n",
        "\n",
        "                # Bias vectors\n",
        "                elif len(vs) == 1:\n",
        "                    new_val = weight_list[idx:idx+vs[0]]\n",
        "                    idx += vs[0]\n",
        "\n",
        "                # Variables (in case of parameter identification setting)\n",
        "                elif len(vs) == 0:\n",
        "                    new_val = weight_list[idx]\n",
        "                    idx += 1\n",
        "\n",
        "                # Assign variables (Casting necessary since scipy requires float64 type)\n",
        "                v.assign(tf.cast(new_val, 'float32'))\n",
        "\n",
        "        def get_loss_and_grad(w):\n",
        "            \"\"\"Function that provides current loss and gradient\n",
        "            w.r.t the trainable variables as vector. This is mandatory\n",
        "            for the LBFGS minimizer from scipy.\"\"\"\n",
        "\n",
        "            # Update weights in model\n",
        "            set_weight_tensor(w)\n",
        "            # Determine value of \\phi and gradient w.r.t. \\theta at w\n",
        "            loss, grad = self.get_grad(X,rt, u)\n",
        "\n",
        "            # Store current loss for callback function\n",
        "            loss = loss.numpy().astype(np.float64)\n",
        "            self.current_loss = loss\n",
        "\n",
        "            # Flatten gradient\n",
        "            grad_flat = []\n",
        "            for g in grad:\n",
        "                grad_flat.extend(g.numpy().flatten())\n",
        "\n",
        "            # Gradient list to array\n",
        "            grad_flat = np.array(grad_flat,dtype=np.float64)\n",
        "\n",
        "            # Return value and gradient of \\phi as tuple\n",
        "            return loss, grad_flat\n",
        "\n",
        "\n",
        "        return scipy.optimize.minimize(fun=get_loss_and_grad,\n",
        "                                       x0=x0,\n",
        "                                       jac=True,\n",
        "                                       method=method,\n",
        "                                       callback=self.callback,\n",
        "                                       **kwargs)\n",
        "\n",
        "    def callback(self, xr=None):\n",
        "        if self.iter % 50 == 0:\n",
        "            print('It {:05d}: loss = {:10.8e}'.format(self.iter,self.current_loss))\n",
        "        self.hist.append(self.current_loss)\n",
        "        self.iter+=1\n",
        "\n",
        "\n",
        "    def plot_solution(self, **kwargs):\n",
        "        N = 600\n",
        "        tspace = np.linspace(self.model.lb[0], self.model.ub[0], N+1)\n",
        "        xspace = np.linspace(self.model.lb[1], self.model.ub[1], N+1)\n",
        "        T, X = np.meshgrid(tspace, xspace)\n",
        "        Xgrid = np.vstack([T.flatten(),X.flatten()]).T\n",
        "        upred = self.model(tf.cast(Xgrid,'float32'))\n",
        "        U = upred.numpy().reshape(N+1,N+1)\n",
        "        fig = plt.figure(figsize=(9,6))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.plot_surface(T, X, U, cmap='viridis', **kwargs)\n",
        "        ax.set_xlabel('$t$')\n",
        "        ax.set_ylabel('$x$')\n",
        "        ax.set_zlabel('$u_\\\\theta(t,x)$')\n",
        "        ax.view_init(35,35)\n",
        "        return ax\n",
        "\n",
        "    def plot_loss_history(self, ax=None):\n",
        "        if not ax:\n",
        "            fig = plt.figure(figsize=(7,5))\n",
        "            ax = fig.add_subplot(111)\n",
        "        ax.semilogy(range(len(self.hist)), self.hist,'k-')\n",
        "        ax.set_xlabel('$n_{epoch}$')\n",
        "        ax.set_ylabel('$\\\\phi^{n_{epoch}}$')\n",
        "        return ax\n",
        "\n",
        "\n",
        "class F_P_PINNSolver(PINNSolver):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def get_r(self):\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # Watch variables representing t and x during this GradientTape\n",
        "            tape.watch(self.t)\n",
        "            tape.watch(self.x)\n",
        "\n",
        "            # Compute current values u(t,x)\n",
        "            u = self.model(tf.stack([self.t[:,0], self.x[:,0]], axis=1))\n",
        "\n",
        "            u_x = tape.gradient(u, self.x)\n",
        "\n",
        "        u_t = tape.gradient(u, self.t)\n",
        "        u_xx = tape.gradient(u_x, self.x)\n",
        "        del tape\n",
        "\n",
        "        return self.fun_r(self.t, self.x, u, u_t, u_x,u_xx)\n",
        "\n",
        "class F_P_PINNIdentification(F_P_PINNSolver):\n",
        "\n",
        "    def fun_r(self, t, x, u, u_t, u_x, u_xx):\n",
        "        \"\"\"Residual of the PDE\"\"\"\n",
        "        # Define residual of the PDE\n",
        "        return u_t + tf.nn.softplus(self.model.lambd)*u_x - 0.5 *(self.model.sig)**2* u_xx\n",
        "        #return u_t + self.model.lambd*u_x - 0.5 *(self.model.sig)**2* u_xx\n",
        "\n",
        "    def callback(self, xr=None):\n",
        "        mu = tf.nn.softplus(self.model.lambd)\n",
        "        sigma= self.model.sig\n",
        "        lambd = mu.numpy()\n",
        "        sig =sigma.numpy()\n",
        "        tt0 =(tf.math.sigmoid(self.model.tt0)*self.minRT).numpy()\n",
        "        #lambd = self.model.lambd.numpy()\n",
        "        self.model.lambd_list.append(lambd)\n",
        "        self.model.sig_list.append(sig)\n",
        "        self.model.tt0_list.append(tt0)\n",
        "        if self.iter % 50 == 0:\n",
        "            print('It {:05d}: loss = {:10.8e} drift = {:10.8e}  sigma = {:10.8e}  t0 = {:10.8e}'.format(self.iter, self.current_loss, lambd,sig, tt0-0.1))\n",
        "\n",
        "        self.hist.append(self.current_loss)\n",
        "        self.iter += 1\n",
        "\n",
        "    def plot_loss_and_param(self, axs=None):\n",
        "        if axs:\n",
        "            ax1, ax2 = axs\n",
        "            self.plot_loss_history(ax1)\n",
        "        else:\n",
        "            ax1 = self.plot_loss_history()\n",
        "            ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "        color = 'tab:blue'\n",
        "        ax2.tick_params(axis='y', labelcolor=color)\n",
        "        ax2.plot(range(len(self.hist)), self.model.lambd_list,'-',color=color)\n",
        "        ax2.set_ylabel('$\\\\lambda^{n_{epoch}}$', color=color)\n",
        "        return (ax1,ax2)\n",
        "    def lambdaf(self):\n",
        "      mu = tf.nn.softplus(self.model.lambd)\n",
        "      return mu.numpy()\n",
        "            #return self.model.lambd.numpy()\n",
        "\n",
        "    def sigmaf(self):\n",
        "        sigma= self.model.sig\n",
        "        return sigma.numpy()\n",
        "\n",
        "    def tt0f(self):\n",
        "            return (tf.math.sigmoid(self.model.tt0)*self.minRT).numpy()\n",
        "\n",
        "num_simulations = 2000  # Number of simulated decision processes\n",
        "time_steps = 20000       # Number of time steps per simulation\n",
        "dt = 0.01               # Time step size\n",
        "drift_rate = 0.5  # Drift rate (v)\n",
        "boundary_separation = 2.0  # Boundary separation (a)\n",
        "starting_point = 0.0     # Starting point (z)\n",
        "noise_std = 1   # Standard deviation of noise (σ)\n",
        "non_Time = 0.3\n",
        "decision_times1,decision_times2 = simulation_ddm_rk4(num_simulations, time_steps,drift_rate, boundary_separation, starting_point, noise_std, dt)\n",
        "rt1 = np.sort(decision_times1) + non_Time\n",
        "rt2 = np.sort(decision_times2) + non_Time\n",
        "\n",
        "\n",
        "RT1 = rt1\n",
        "RT2 = rt2\n",
        "threshold = 2\n",
        "epoch = 50000\n",
        "# Mesh Grid\n",
        "N = 100\n",
        "xmax = threshold\n",
        "#xmax = threshold\n",
        "max1 = max(RT1)\n",
        "xmin = -xmax\n",
        "lb = [0, xmin]\n",
        "maxrt1 = max(RT1)\n",
        "maxrt2 = max(RT2)\n",
        "ub = [max(maxrt1,maxrt2),xmax]\n",
        "tspace = np.linspace(lb[0], ub[0], N + 1)\n",
        "xspace = np.linspace(lb[1], ub[1], N + 1)\n",
        "T, X = np.meshgrid(tspace, xspace)\n",
        "Xgrid = np.vstack([T.flatten(),X.flatten()]).T\n",
        "#tspace = np.random.uniform(lb[0], ub[0],N)\n",
        "#xspace = np.random.uniform(lb[1], ub[1],N)\n",
        "#Xgrid = np.vstack([tspace,xspace]).T\n",
        "Xgrid = tf.constant(Xgrid,'float32')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Boundary data\n",
        "N_b = 500\n",
        "t_b = tf.random.uniform((N_b,1), lb[0], ub[0], dtype='float32')\n",
        "x_b = lb[1] + (ub[1] - lb[1]) * tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype='float32')\n",
        "X_b = tf.concat([t_b, x_b], axis=1)\n",
        "X_b\n",
        "u_b = tf.zeros(tf.shape(x_b),'float32')\n",
        "\n",
        "\n",
        "delta = 7.8*10**(-2)\n",
        "\n",
        "# direct’s delta function\n",
        "def ddf(x,delta,x0):\n",
        "    return 1/(2*np.sqrt(np.pi*delta))*tf.math.exp(-((x-x0)**2)/(4*delta))\n",
        "\n",
        "# Define initial condition\n",
        "def fun_u_0(x):\n",
        "    return ddf(x,delta,0)\n",
        "\n",
        "\n",
        "N_0 = 50\n",
        "t_0 = tf.ones((N_0,1), dtype='float32')*lb[0]    #t = 0\n",
        "x_0 = np.linspace(lb[1], ub[1], N_0-1, dtype='float32')\n",
        "x_0 = np.asarray(list(x_0) + [0.0])\n",
        "x_0 = np.sort(x_0)\n",
        "#u_0 = np.where(x_0==0,1,0)\n",
        "#uu_0 = 2.5*smooth(u_0, 1)\n",
        "x_0 = tf.convert_to_tensor(x_0,dtype='float32')\n",
        "x_0 = tf.reshape(x_0 ,[N_0,1])\n",
        "u_0 = fun_u_0(x_0)\n",
        "#plt.plot(x_0,u_0)\n",
        "\n",
        "X_0 = tf.concat([t_0, x_0], axis=1)\n",
        "\n",
        "X_param = [X_0,X_b]\n",
        "u_param = [u_0, u_b]\n",
        "\n",
        "# dtype in Tensorflow\n",
        "lb = tf.constant([0, xmin],dtype='float32')\n",
        "ub = tf.constant([max(maxrt1,maxrt2),xmax],dtype='float32')\n",
        "\n",
        "# Initialize model\n",
        "model = PINNIdentificationNet(lb, ub, num_hidden_layers=4,num_neurons_per_layer=30,\n",
        "                                        activation='tanh',kernel_initializer='glorot_normal')\n",
        "model.build(input_shape=(None,2))\n",
        "# Initilize solver\n",
        "f_p_Identification = F_P_PINNIdentification(model, Xgrid)\n",
        "\n",
        "# Choose step sizes aka learning rate\n",
        "lr =  tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,15000],[0.01,0.001,0.0005])\n",
        "\n",
        "# Solve with Adam optimizer\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=lr) ####################################\n",
        "\n",
        "# Start timer\n",
        "t0 = time()\n",
        "f_p_Identification.solve_with_TFoptimizer(optim, X_param ,xmax,RT1,RT2, u_param, N=epoch)\n",
        "print('\\nComputation time: {} seconds'.format(time()-t0))\n",
        "#f_p_Identification.plot_solution()\n",
        "#f_p_Identification.plot_loss_and_param()\n",
        "print(drift_rate,noise_std,non_Time)\n",
        "print(f_p_Identification.lambdaf(),f_p_Identification.sigmaf(),f_p_Identification.tt0f()-0.1)\n",
        "\n",
        "\n",
        "\n",
        "M = 500\n",
        "DTYPE='float32'\n",
        "tspace = np.linspace(0, ub[0], M + 1)\n",
        "xspace = np.ones((M+1))*ub[1]\n",
        "X = np.zeros((M+1,2))\n",
        "X[:,0] = tspace\n",
        "X[:,1] = xspace\n",
        "X = tf.constant(X,DTYPE)\n",
        "p_i =  model(X)\n",
        "\n",
        "xspace1 = np.ones((M+1))*(ub[1]-0.02)\n",
        "X1 = np.zeros((M+1,2))\n",
        "X1[:,0] = tspace\n",
        "X1[:,1] = xspace1\n",
        "X1 = tf.constant(X1,DTYPE)\n",
        "p_ii =  model(X1)\n",
        "\n",
        "xspace2 = np.ones((M+1))*(ub[1]-0.04)\n",
        "X2 = np.zeros((M+1,2))\n",
        "X2[:,0] = tspace\n",
        "X2[:,1] = xspace2\n",
        "X2 = tf.constant(X2,DTYPE)\n",
        "p_iii =  model(X2)\n",
        "p_x = (3*p_i-4*p_ii+p_iii)/(2*0.02)\n",
        "J1 = f_p_Identification.lambdaf()* p_i -0.5*f_p_Identification.sigmaf()**2*p_x\n",
        "\n",
        "\n",
        "\n",
        "def f(v,a,t):\n",
        "      return (a/np.sqrt(2*np.pi*t**3)*np.exp(-(v*t-a)**2/(2*t)))\n",
        "F1 = []\n",
        "C = []\n",
        "for rt in np.sort(tspace):\n",
        "  F1.append(f(drift_rate,boundary_separation,rt))\n",
        "  C.append(np.trapz(F1))\n",
        "\n",
        "kde1 = gaussian_kde(rt1)\n",
        "kde2 = gaussian_kde(rt2)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# قالب فونت یکسان\n",
        "font = {'family': 'serif',\n",
        "        'color':  'black',\n",
        "        'weight': 'normal',\n",
        "        'size': 16,\n",
        "        }\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8), dpi=90)\n",
        "\n",
        "\n",
        "plt.plot(tspace + 0.07, J1, label='Approximated', color='black')\n",
        "plt.plot(tspace, np.asarray(F1), label='Exact', color='black', linestyle='--')\n",
        "plt.plot(rt1 - 0.3, len(rt1) * kde1(rt1)[:, np.newaxis] / (len(rt1) + len(rt2)),\n",
        "         label='Data', color='red', linestyle='-.')\n",
        "\n",
        "plt.xlabel('Time', fontdict=font)\n",
        "plt.ylabel('First passage time', fontdict=font)\n",
        "\n",
        "\n",
        "plt.legend(fontsize=18)\n",
        "\n",
        "\n",
        "plt.tick_params(axis='both', which='major', labelsize=14)\n",
        "\n",
        "\n",
        "plt.savefig(\"ex1-10.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "F2 = []\n",
        "C = []\n",
        "for rt in np.sort(tspace):\n",
        "  F2.append(f(drift_rate,-boundary_separation,rt))\n",
        "  C.append(np.trapz(F2))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# قالب فونت یکسان\n",
        "font = {'family': 'serif',\n",
        "        'color':  'black',   # مشکی شدن لیبل‌ها\n",
        "        'weight': 'normal',\n",
        "        'size': 16,\n",
        "        }\n",
        "\n",
        "# اندازه تصویر\n",
        "plt.figure(figsize=(12, 8), dpi=90)\n",
        "\n",
        "# تولید داده‌ها مشابه کد اصلی\n",
        "M = 500\n",
        "DTYPE='float32'\n",
        "tspace = np.linspace(0, ub[0], M + 1)\n",
        "xspace = np.ones((M+1))*lb[1]\n",
        "X = np.zeros((M+1,2))\n",
        "X[:,0] = tspace\n",
        "X[:,1] = xspace\n",
        "X = tf.constant(X,DTYPE)\n",
        "p_i = model(X)\n",
        "\n",
        "xspace1 = np.ones((M+1))*(lb[1]+0.05)\n",
        "X1 = np.zeros((M+1,2))\n",
        "X1[:,0] = tspace\n",
        "X1[:,1] = xspace1\n",
        "X1 = tf.constant(X1,DTYPE)\n",
        "p_ii = model(X1)\n",
        "\n",
        "xspace2 = np.ones((M+1))*(lb[1]+.1)\n",
        "X2 = np.zeros((M+1,2))\n",
        "X2[:,0] = tspace\n",
        "X2[:,1] = xspace2\n",
        "X2 = tf.constant(X2,DTYPE)\n",
        "p_iii = model(X2)\n",
        "\n",
        "p_x = (-3*p_i+4*p_ii-p_iii)/(2*0.05)\n",
        "J2 = f_p_Identification.lambdaf()* p_i -0.5*f_p_Identification.sigmaf()**2*p_x\n",
        "\n",
        "plt.plot(tspace +.05 , -J2, label='Approximated',color='black')\n",
        "plt.plot(tspace, -np.asarray(F2), label='Exact', linestyle='--', color='black')\n",
        "plt.plot(rt2 - f_p_Identification.tt0f(), len(rt2) * kde2(rt2)[:, np.newaxis] / (len(rt1) + len(rt2)),\n",
        "         label='Data', color='red', linestyle='-.')\n",
        "\n",
        "plt.xlabel('Time', fontdict=font)\n",
        "plt.ylabel('First passage time', fontdict=font)\n",
        "\n",
        "\n",
        "plt.legend(fontsize=18)\n",
        "\n",
        "\n",
        "plt.tick_params(axis='both', which='major', labelsize=14)\n",
        "\n",
        "plt.savefig(\"ex1-11.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Theoretical-Informed Neural Network (TINN) for Parameter Estimation in Drift Diffusion Model\n",
        "\n",
        "Q1 Journal Paper: Parameter estimation in cognitive models using physics-informed deep learning\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import scipy.optimize\n",
        "import pandas as pd\n",
        "from scipy.stats import gaussian_kde\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# =============================================================================\n",
        "# Configuration\n",
        "# =============================================================================\n",
        "\n",
        "# Simulation parameters\n",
        "NUM_SIMULATIONS = 2000\n",
        "TIME_STEPS = 20000\n",
        "DT = 0.01\n",
        "NON_DECISION_TIME = 0.3\n",
        "\n",
        "# True parameters for DDM\n",
        "DRIFT_RATE = 0.5\n",
        "BOUNDARY_SEPARATION = 2.0\n",
        "STARTING_POINT = 0.0\n",
        "NOISE_STD = 1.0\n",
        "\n",
        "# TINN training parameters\n",
        "NUM_EPOCHS = 50000\n",
        "NUM_HIDDEN_LAYERS = 4\n",
        "NUM_NEURONS_PER_LAYER = 30\n",
        "THRESHOLD = 2.0\n",
        "\n",
        "# =============================================================================\n",
        "# DDM Simulation\n",
        "# =============================================================================\n",
        "\n",
        "def simulate_ddm_rk4(num_simulations, time_steps, drift_rate, boundary_separation, starting_point, noise_std, dt):\n",
        "    decision_times1 = []\n",
        "    decision_times2 = []\n",
        "    noise = np.random.normal(0, noise_std, (num_simulations, time_steps))\n",
        "\n",
        "    for i in range(num_simulations):\n",
        "        decision_variable = starting_point\n",
        "        for t in range(time_steps):\n",
        "            k1 = drift_rate * dt + noise[i, t] * np.sqrt(dt)\n",
        "            k2 = drift_rate * dt + noise[i, t] * np.sqrt(dt)\n",
        "            k3 = drift_rate * dt + noise[i, t] * np.sqrt(dt)\n",
        "            k4 = drift_rate * dt + noise[i, t] * np.sqrt(dt)\n",
        "            decision_variable += (k1 + 2*k2 + 2*k3 + k4) / 6\n",
        "\n",
        "            if decision_variable >= boundary_separation:\n",
        "                decision_times1.append(t * dt)\n",
        "                break\n",
        "            elif decision_variable <= -boundary_separation:\n",
        "                decision_times2.append(t * dt)\n",
        "                break\n",
        "    return np.array(decision_times1), np.array(decision_times2)\n",
        "\n",
        "# =============================================================================\n",
        "# TINN Architecture\n",
        "# =============================================================================\n",
        "\n",
        "class TINN_NeuralNet(tf.keras.Model):\n",
        "    def __init__(self, lb, ub, output_dim=1, num_hidden_layers=5, num_neurons_per_layer=50, activation='tanh', kernel_initializer='glorot_normal', **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.output_dim = output_dim\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer, activation=tf.keras.activations.get(activation), kernel_initializer=kernel_initializer) for _ in range(self.num_hidden_layers-1)]\n",
        "        self.hidden1 = tf.keras.layers.Dense(num_neurons_per_layer, activation='softplus')\n",
        "        self.out = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, X):\n",
        "        Z = X\n",
        "        for i in range(self.num_hidden_layers-1):\n",
        "            Z = self.hidden[i](Z)\n",
        "        return self.out(Z)\n",
        "\n",
        "class TINNIdentificationNet(TINN_NeuralNet):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.lambd = self.add_weight(name=\"lambd_raw\", initializer=\"ones\", trainable=True, dtype=tf.float32)\n",
        "        self.sig = self.add_weight(name=\"sig\", initializer=\"ones\", trainable=True, dtype=tf.float32)\n",
        "        self.tt0 = self.add_weight(name=\"tt0\", initializer=\"ones\", trainable=True, dtype=tf.float32)\n",
        "        self.lambd_list = []\n",
        "        self.sig_list = []\n",
        "        self.tt0_list = []\n",
        "\n",
        "# =============================================================================\n",
        "# TINN Solver\n",
        "# =============================================================================\n",
        "\n",
        "class TINNSolver():\n",
        "    def __init__(self, model, X_r):\n",
        "        self.model = model\n",
        "        self.t = X_r[:,0:1]\n",
        "        self.x = X_r[:,1:2]\n",
        "        self.hist = []\n",
        "        self.iter = 0\n",
        "\n",
        "    def get_r(self):\n",
        "        with tf.GradientTape(persistent=False) as tape:\n",
        "            tape.watch(self.t)\n",
        "            tape.watch(self.x)\n",
        "            u = self.model(tf.stack([self.t[:,0], self.x[:,0]], axis=1))\n",
        "            u_x = tape.gradient(u, self.x)\n",
        "        u_t = tape.gradient(u, self.t)\n",
        "        u_xx = tape.gradient(u_x, self.x)\n",
        "        del tape\n",
        "        return self.fun_r(self.t, self.x, u, u_t, u_x, u_xx)\n",
        "\n",
        "    def loss_fn(self, X, xmax, rt1, rt2, u):\n",
        "        r = self.get_r()\n",
        "        phi_r = tf.reduce_mean(tf.square(r))\n",
        "        loss_r = phi_r\n",
        "\n",
        "        u_pred = self.model(X[0])\n",
        "        loss_0 = tf.reduce_mean(tf.square(u[0] - u_pred))\n",
        "\n",
        "        u_pred = self.model(X[1])\n",
        "        loss_b = tf.reduce_mean(tf.square(u[1] - u_pred))\n",
        "\n",
        "        tspace1 = np.sort(rt1)\n",
        "        tspace2 = np.sort(rt2)\n",
        "        tspace_tf1 = tf.constant(tspace1.reshape((len(rt1),1)), 'float32')\n",
        "        minRT1 = min(rt1)\n",
        "        minRT2 = min(rt2)\n",
        "        self.minRT = min(minRT1, minRT2)\n",
        "        t1 = tspace_tf1 - tf.math.sigmoid(self.model.tt0)*self.minRT\n",
        "\n",
        "        xspace = np.ones((len(tspace1),1))*xmax\n",
        "        xtf = tf.constant(xspace, 'float32')\n",
        "        X_bound = tf.concat([t1, xtf], 1)\n",
        "        p_i = self.model(X_bound)\n",
        "\n",
        "        xspace1 = np.ones((len(tspace1),1))*(xmax-0.02)\n",
        "        xtf1 = tf.constant(xspace1, 'float32')\n",
        "        X1 = tf.concat([t1, xtf1], 1)\n",
        "        p_ii = self.model(X1)\n",
        "\n",
        "        xspace2 = np.ones((len(tspace1),1))*(xmax-0.04)\n",
        "        xtf2 = tf.constant(xspace2, 'float32')\n",
        "        X2 = tf.concat([t1, xtf2], 1)\n",
        "        p_iii = self.model(X2)\n",
        "        p_x = (3*p_i - 4*p_ii + p_iii)/(2*0.02)\n",
        "\n",
        "        J1 = tf.nn.softplus(self.model.lambd)* p_i - 0.5*(self.model.sig)**2*p_x\n",
        "        kde1 = gaussian_kde(tspace1)\n",
        "        p_kde1 = len(tspace1)*kde1(tspace1)[:, np.newaxis]/(len(tspace2)+len(tspace1))\n",
        "        p_kde_tensor1 = tf.convert_to_tensor(p_kde1, dtype=tf.float32)\n",
        "        KDE_loss1 = tf.reduce_mean(tf.square(J1 - p_kde_tensor1))\n",
        "\n",
        "        tspace_tf2 = tf.constant(tspace2.reshape((len(rt2),1)), 'float32')\n",
        "        t2 = tspace_tf2 - tf.math.sigmoid(self.model.tt0)*self.minRT\n",
        "\n",
        "        xspace20 = np.ones((len(tspace2),1))*(-xmax)\n",
        "        xtf20 = tf.constant(xspace20, 'float32')\n",
        "        X20 = tf.concat([t2, xtf20], 1)\n",
        "        p_i2 = self.model(X20)\n",
        "\n",
        "        xspace21 = np.ones((len(tspace2),1))*(-xmax+0.02)\n",
        "        xtf21 = tf.constant(xspace21, 'float32')\n",
        "        X21 = tf.concat([t2, xtf21], 1)\n",
        "        p_ii2 = self.model(X21)\n",
        "\n",
        "        xspace22 = np.ones((len(tspace2),1))*(-xmax+0.04)\n",
        "        xtf22 = tf.constant(xspace22, 'float32')\n",
        "        X22 = tf.concat([t2, xtf22], 1)\n",
        "        p_iii2 = self.model(X22)\n",
        "        p_x2 = (-3*p_i2 + 4*p_ii2 - p_iii2)/(2*0.02)\n",
        "\n",
        "        J2 = tf.nn.softplus(self.model.lambd)* p_i2 - 0.5*(self.model.sig)**2*p_x2\n",
        "        kde2 = gaussian_kde(tspace2)\n",
        "        p_kde2 = -len(tspace2)*kde2(tspace2)[:, np.newaxis]/(len(tspace2)+len(tspace1))\n",
        "        p_kde_tensor2 = tf.convert_to_tensor(p_kde2, dtype=tf.float32)\n",
        "        KDE_loss2 = tf.reduce_mean(tf.square(J2 - p_kde_tensor2))\n",
        "\n",
        "        return 100*(loss_r + loss_0 + loss_b) + KDE_loss1 + KDE_loss2\n",
        "\n",
        "    def get_grad(self, X, xmax, rt1, rt2, u):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.loss_fn(X, xmax, rt1, rt2, u)\n",
        "        g = tape.gradient(loss, self.model.trainable_variables)\n",
        "        del tape\n",
        "        return loss, g\n",
        "\n",
        "    def solve_with_TFoptimizer(self, optimizer, X, xmax, rt1, rt2, u, N=1001):\n",
        "        @tf.function\n",
        "        def train_step():\n",
        "            loss, grad_theta = self.get_grad(X, xmax, rt1, rt2, u)\n",
        "            optimizer.apply_gradients(zip(grad_theta, self.model.trainable_variables))\n",
        "            return loss\n",
        "\n",
        "        for i in range(N):\n",
        "            loss = train_step()\n",
        "            self.current_loss = loss.numpy()\n",
        "            self.callback()\n",
        "\n",
        "    def callback(self, xr=None):\n",
        "        if self.iter % 50 == 0:\n",
        "            print('It {:05d}: loss = {:10.8e}'.format(self.iter, self.current_loss))\n",
        "        self.hist.append(self.current_loss)\n",
        "        self.iter += 1\n",
        "\n",
        "# =============================================================================\n",
        "# Fokker-Planck TINN Solver\n",
        "# =============================================================================\n",
        "\n",
        "class F_P_TINNSolver(TINNSolver):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def get_r(self):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            tape.watch(self.t)\n",
        "            tape.watch(self.x)\n",
        "            u = self.model(tf.stack([self.t[:,0], self.x[:,0]], axis=1))\n",
        "            u_x = tape.gradient(u, self.x)\n",
        "        u_t = tape.gradient(u, self.t)\n",
        "        u_xx = tape.gradient(u_x, self.x)\n",
        "        del tape\n",
        "        return self.fun_r(self.t, self.x, u, u_t, u_x, u_xx)\n",
        "\n",
        "class F_P_TINNIdentification(F_P_TINNSolver):\n",
        "    def fun_r(self, t, x, u, u_t, u_x, u_xx):\n",
        "        return u_t + tf.nn.softplus(self.model.lambd)*u_x - 0.5*(self.model.sig)**2*u_xx\n",
        "\n",
        "    def callback(self, xr=None):\n",
        "        mu = tf.nn.softplus(self.model.lambd)\n",
        "        sigma = self.model.sig\n",
        "        lambd = mu.numpy()\n",
        "        sig = sigma.numpy()\n",
        "        tt0 = (tf.math.sigmoid(self.model.tt0)*self.minRT).numpy()\n",
        "        self.model.lambd_list.append(lambd)\n",
        "        self.model.sig_list.append(sig)\n",
        "        self.model.tt0_list.append(tt0)\n",
        "        if self.iter % 50 == 0:\n",
        "            print('It {:05d}: loss = {:10.8e} drift = {:10.8e} sigma = {:10.8e} t0 = {:10.8e}'.format(self.iter, self.current_loss, lambd, sig, tt0-0.1))\n",
        "        self.hist.append(self.current_loss)\n",
        "        self.iter += 1\n",
        "\n",
        "    def lambdaf(self):\n",
        "        mu = tf.nn.softplus(self.model.lambd)\n",
        "        return mu.numpy()\n",
        "\n",
        "    def sigmaf(self):\n",
        "        return self.model.sig.numpy()\n",
        "\n",
        "    def tt0f(self):\n",
        "        return (tf.math.sigmoid(self.model.tt0)*self.minRT).numpy()\n",
        "\n",
        "# =============================================================================\n",
        "# Main Execution\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"TINN Parameter Estimation for Drift Diffusion Model\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Generate synthetic data\n",
        "    print(\"Generating synthetic DDM data...\")\n",
        "    decision_times1, decision_times2 = simulate_ddm_rk4(NUM_SIMULATIONS, TIME_STEPS, DRIFT_RATE, BOUNDARY_SEPARATION, STARTING_POINT, NOISE_STD, DT)\n",
        "    rt1 = np.sort(decision_times1) + NON_DECISION_TIME\n",
        "    rt2 = np.sort(decision_times2) + NON_DECISION_TIME\n",
        "    print(f\"Generated {len(rt1)} upper and {len(rt2)} lower boundary crossings\")\n",
        "\n",
        "    # Prepare domain and training data\n",
        "    xmax = THRESHOLD\n",
        "    xmin = -xmax\n",
        "    maxrt1 = max(rt1)\n",
        "    maxrt2 = max(rt2)\n",
        "    lb = tf.constant([0, xmin], dtype='float32')\n",
        "    ub = tf.constant([max(maxrt1, maxrt2), xmax], dtype='float32')\n",
        "\n",
        "    # Collocation points\n",
        "    N = 100\n",
        "    tspace = np.linspace(lb[0], ub[0], N + 1)\n",
        "    xspace = np.linspace(lb[1], ub[1], N + 1)\n",
        "    T, X = np.meshgrid(tspace, xspace)\n",
        "    Xgrid = np.vstack([T.flatten(), X.flatten()]).T\n",
        "    Xgrid = tf.constant(Xgrid, 'float32')\n",
        "\n",
        "    # Boundary conditions\n",
        "    def ddf(x, delta, x0):\n",
        "        return 1/(2*np.sqrt(np.pi*delta))*tf.math.exp(-((x-x0)**2)/(4*delta))\n",
        "\n",
        "    def fun_u_0(x):\n",
        "        return ddf(x, 7.8e-2, 0)\n",
        "\n",
        "    N_b = 500\n",
        "    t_b = tf.random.uniform((N_b,1), lb[0], ub[0], dtype='float32')\n",
        "    x_b = lb[1] + (ub[1] - lb[1]) * tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype='float32')\n",
        "    X_b = tf.concat([t_b, x_b], axis=1)\n",
        "    u_b = tf.zeros(tf.shape(x_b), 'float32')\n",
        "\n",
        "    N_0 = 50\n",
        "    t_0 = tf.ones((N_0,1), dtype='float32')*lb[0]\n",
        "    x_0 = np.linspace(lb[1], ub[1], N_0-1, dtype='float32')\n",
        "    x_0 = np.asarray(list(x_0) + [0.0])\n",
        "    x_0 = np.sort(x_0)\n",
        "    x_0 = tf.convert_to_tensor(x_0, dtype='float32')\n",
        "    x_0 = tf.reshape(x_0, [N_0,1])\n",
        "    u_0 = fun_u_0(x_0)\n",
        "    X_0 = tf.concat([t_0, x_0], axis=1)\n",
        "\n",
        "    X_param = [X_0, X_b]\n",
        "    u_param = [u_0, u_b]\n",
        "\n",
        "    # Initialize and train TINN\n",
        "    model = TINNIdentificationNet(lb, ub, num_hidden_layers=NUM_HIDDEN_LAYERS, num_neurons_per_layer=NUM_NEURONS_PER_LAYER, activation='tanh', kernel_initializer='glorot_normal')\n",
        "\n",
        "    f_p_Identification = F_P_TINNIdentification(model, Xgrid)\n",
        "\n",
        "    lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,15000], [0.01,0.001,0.0005])\n",
        "    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    print(\"Starting TINN training...\")\n",
        "    t0 = time()\n",
        "    f_p_Identification.solve_with_TFoptimizer(optim, X_param, xmax, rt1, rt2, u_param, N=NUM_EPOCHS)\n",
        "    print('\\nComputation time: {} seconds'.format(time()-t0))\n",
        "\n",
        "    # Results\n",
        "    print(\"\\nTrue parameters: Drift = {:.4f}, Noise = {:.4f}, Non-decision = {:.4f}\".format(DRIFT_RATE, NOISE_STD, NON_DECISION_TIME))\n",
        "    print(\"Estimated parameters: Drift = {:.4f}, Noise = {:.4f}, Non-decision = {:.4f}\".format(f_p_Identification.lambdaf(), f_p_Identification.sigmaf(), f_p_Identification.tt0f()-0.1))\n",
        "\n",
        "    # Visualization\n",
        "    plot_results(model, f_p_Identification, rt1, rt2, lb, ub, DRIFT_RATE, BOUNDARY_SEPARATION)\n",
        "\n",
        "def plot_results(model, solver, rt1, rt2, lb, ub, drift_rate, boundary_separation):\n",
        "    # Analytical solution\n",
        "    def f(v, a, t):\n",
        "        return (a/np.sqrt(2*np.pi*t**3)*np.exp(-(v*t-a)**2/(2*t)))\n",
        "\n",
        "    # Upper boundary\n",
        "    M = 500\n",
        "    tspace = np.linspace(0, ub[0], M + 1)\n",
        "    xspace = np.ones((M+1))*ub[1]\n",
        "    X = np.zeros((M+1,2))\n",
        "    X[:,0] = tspace\n",
        "    X[:,1] = xspace\n",
        "    X = tf.constant(X, 'float32')\n",
        "    p_i = model(X)\n",
        "\n",
        "    xspace1 = np.ones((M+1))*(ub[1]-0.02)\n",
        "    X1 = np.zeros((M+1,2))\n",
        "    X1[:,0] = tspace\n",
        "    X1[:,1] = xspace1\n",
        "    X1 = tf.constant(X1, 'float32')\n",
        "    p_ii = model(X1)\n",
        "\n",
        "    xspace2 = np.ones((M+1))*(ub[1]-0.04)\n",
        "    X2 = np.zeros((M+1,2))\n",
        "    X2[:,0] = tspace\n",
        "    X2[:,1] = xspace2\n",
        "    X2 = tf.constant(X2, 'float32')\n",
        "    p_iii = model(X2)\n",
        "    p_x = (3*p_i - 4*p_ii + p_iii)/(2*0.02)\n",
        "    J1 = solver.lambdaf()* p_i - 0.5*solver.sigmaf()**2*p_x\n",
        "\n",
        "    F1 = [f(drift_rate, boundary_separation, rt) for rt in np.sort(tspace)]\n",
        "    kde1 = gaussian_kde(rt1)\n",
        "\n",
        "    font = {'family': 'serif', 'color': 'black', 'weight': 'normal', 'size': 16}\n",
        "\n",
        "    plt.figure(figsize=(12, 8), dpi=90)\n",
        "    plt.plot(tspace + 0.07, J1, label='TINN Approximation', color='black')\n",
        "    plt.plot(tspace, np.asarray(F1), label='Analytical Solution', color='black', linestyle='--')\n",
        "    plt.plot(rt1 - 0.3, len(rt1)*kde1(rt1)[:, np.newaxis]/(len(rt1)+len(rt2)), label='Empirical Data', color='red', linestyle='-.')\n",
        "    plt.xlabel('Time', fontdict=font)\n",
        "    plt.ylabel('First Passage Density', fontdict=font)\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig(\"tinn_upper_boundary.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nAnalysis completed successfully!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "LtLC916YAfuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VL7wPssWAjNn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}