{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEFurqPXW6et"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Theoretical-Informed Neural Network (TINN) for Parameter Estimation\n",
        "in Ornstein-Uhlenbeck Drift Diffusion Model (OU-DDM)\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Configuration and Constants\n",
        "# =============================================================================\n",
        "\n",
        "# True parameters for OU-DDM\n",
        "DRIFT_0 = 0.6\n",
        "BETA = 0.3\n",
        "BOUNDARY_SEPARATION = 2.0\n",
        "STARTING_POINT = 0.0\n",
        "NOISE_STD = 1.0\n",
        "NON_DECISION_TIME = 0.7\n",
        "\n",
        "# Simulation parameters\n",
        "NUM_SIMULATIONS = 5000\n",
        "TIME_STEPS = 20000\n",
        "DT = 0.01\n",
        "\n",
        "# TINN training parameters\n",
        "NUM_EPOCHS = 30000\n",
        "NUM_HIDDEN_LAYERS = 4\n",
        "NUM_NEURONS_PER_LAYER = 50\n",
        "\n",
        "# Domain parameters\n",
        "XMIN = -2.0\n",
        "XMAX = 2.0\n",
        "N_COLLOCATION = 200\n",
        "N_BOUNDARY = 100\n",
        "N_INITIAL = 50\n",
        "\n",
        "# Physical parameters\n",
        "DELTA = 7.8e-2\n",
        "\n",
        "# Loss weights\n",
        "LAMBDA_PDE = 1.0\n",
        "LAMBDA_IC = 10.0\n",
        "LAMBDA_BC = 10.0\n",
        "LAMBDA_DATA = 100.0\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Ornstein-Uhlenbeck DDM Simulation\n",
        "# =============================================================================\n",
        "\n",
        "def drift_ou(drift0, beta, xx):\n",
        "    \"\"\"Drift function for Ornstein-Uhlenbeck process.\"\"\"\n",
        "    return drift0 - beta * xx\n",
        "\n",
        "\n",
        "def simulate_ou_ddm_rk4(num_simulations, time_steps, boundary_separation,\n",
        "                       starting_point, noise_std, dt, drift0, beta):\n",
        "    \"\"\"Simulate OU-DDM using RK4 integration for improved accuracy.\"\"\"\n",
        "    decision_times_upper = []\n",
        "    decision_times_lower = []\n",
        "    trajectories_upper = []\n",
        "    trajectories_lower = []\n",
        "\n",
        "    for i in range(num_simulations):\n",
        "        decision_variable = starting_point\n",
        "        trajectory = [decision_variable]\n",
        "\n",
        "        for t in range(1, time_steps):\n",
        "            current_position = decision_variable\n",
        "\n",
        "            # RK4 method for stochastic integration\n",
        "            k1 = drift_ou(drift0, beta, current_position) * dt + np.random.normal(0, noise_std) * np.sqrt(dt)\n",
        "            k2 = drift_ou(drift0, beta, current_position + k1/2) * dt + np.random.normal(0, noise_std) * np.sqrt(dt)\n",
        "            k3 = drift_ou(drift0, beta, current_position + k2/2) * dt + np.random.normal(0, noise_std) * np.sqrt(dt)\n",
        "            k4 = drift_ou(drift0, beta, current_position + k3) * dt + np.random.normal(0, noise_std) * np.sqrt(dt)\n",
        "\n",
        "            decision_variable += (k1 + 2*k2 + 2*k3 + k4) / 6\n",
        "            trajectory.append(decision_variable)\n",
        "\n",
        "            if decision_variable >= boundary_separation:\n",
        "                decision_times_upper.append(t * dt)\n",
        "                trajectories_upper.append(trajectory)\n",
        "                break\n",
        "            elif decision_variable <= -boundary_separation:\n",
        "                decision_times_lower.append(t * dt)\n",
        "                trajectories_lower.append(trajectory)\n",
        "                break\n",
        "        else:\n",
        "            # If no decision is made\n",
        "            pass\n",
        "\n",
        "    return (np.array(decision_times_upper), np.array(decision_times_lower),\n",
        "            trajectories_upper, trajectories_lower)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Neural Network Architecture\n",
        "# =============================================================================\n",
        "\n",
        "class PINN_NeuralNet(tf.keras.Model):\n",
        "    \"\"\"Base architecture for Physics-Informed Neural Network.\"\"\"\n",
        "\n",
        "    def __init__(self, lb, ub, output_dim=1, num_hidden_layers=5,\n",
        "                 num_neurons_per_layer=50, activation='tanh',\n",
        "                 kernel_initializer='glorot_normal', **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.output_dim = output_dim\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "\n",
        "        # Define NN architecture\n",
        "        self.hidden_layers = []\n",
        "        for _ in range(num_hidden_layers):\n",
        "            self.hidden_layers.append(\n",
        "                tf.keras.layers.Dense(num_neurons_per_layer,\n",
        "                                    activation=activation,\n",
        "                                    kernel_initializer=kernel_initializer)\n",
        "            )\n",
        "        self.out = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, X):\n",
        "        \"\"\"Forward-pass through neural network.\"\"\"\n",
        "        Z = X\n",
        "        for layer in self.hidden_layers:\n",
        "            Z = layer(Z)\n",
        "        return self.out(Z)\n",
        "\n",
        "\n",
        "class PINNIdentificationNet(PINN_NeuralNet):\n",
        "    \"\"\"PINN with trainable parameters for OU-DDM system identification.\"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Initialize trainable parameters with better initial values\n",
        "        self.lambd0 = tf.Variable(0.5, name=\"lambd0\", trainable=True, dtype=tf.float32)\n",
        "        self.beta = tf.Variable(0.2, name=\"beta\", trainable=True, dtype=tf.float32)\n",
        "        self.sig = tf.Variable(0.8, name=\"sig\", trainable=True, dtype=tf.float32)\n",
        "        self.tt0 = tf.Variable(0.5, name=\"tt0\", trainable=True, dtype=tf.float32)\n",
        "\n",
        "        # History tracking\n",
        "        self.lambd0_list = []\n",
        "        self.beta_list = []\n",
        "        self.sig_list = []\n",
        "        self.tt0_list = []\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# PINN Solver\n",
        "# =============================================================================\n",
        "\n",
        "class F_P_PINNIdentification:\n",
        "    \"\"\"Fokker-Planck PINN for OU-DDM parameter identification.\"\"\"\n",
        "\n",
        "    def __init__(self, model, X_r):\n",
        "        self.model = model\n",
        "        self.t = X_r[:, 0:1]\n",
        "        self.x = X_r[:, 1:2]\n",
        "        self.hist = []\n",
        "        self.iter = 0\n",
        "        self.minRT = None\n",
        "\n",
        "    def get_r(self):\n",
        "        \"\"\"Compute PDE residual using automatic differentiation.\"\"\"\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            tape.watch(self.t)\n",
        "            tape.watch(self.x)\n",
        "\n",
        "            # Compute solution\n",
        "            u = self.model(tf.stack([self.t[:, 0], self.x[:, 0]], axis=1))\n",
        "\n",
        "            # Compute gradients\n",
        "            u_x = tape.gradient(u, self.x)\n",
        "            u_xx = tape.gradient(u_x, self.x)\n",
        "            u_t = tape.gradient(u, self.t)\n",
        "\n",
        "        # Fokker-Planck equation for OU process\n",
        "        drift = self.model.lambd0 - self.model.beta * self.x\n",
        "        residual = u_t + drift * u_x - 0.5 * (self.model.sig)**2 * u_xx - self.model.beta * u\n",
        "\n",
        "        del tape\n",
        "        return residual\n",
        "\n",
        "    def loss_fn(self, X, xmax, rt1, rt2, u_data):\n",
        "        \"\"\"Compute total loss function.\"\"\"\n",
        "        # Set minRT for the first time\n",
        "        if self.minRT is None:\n",
        "            self.minRT = min(np.min(rt1), np.min(rt2))\n",
        "\n",
        "        # 1. PDE residual loss\n",
        "        r = self.get_r()\n",
        "        loss_pde = tf.reduce_mean(tf.square(r))\n",
        "\n",
        "        # 2. Initial condition loss\n",
        "        u_pred_0 = self.model(X[0])\n",
        "        loss_ic = tf.reduce_mean(tf.square(u_data[0] - u_pred_0))\n",
        "\n",
        "        # 3. Boundary condition loss\n",
        "        u_pred_b = self.model(X[1])\n",
        "        loss_bc = tf.reduce_mean(tf.square(u_data[1] - u_pred_b))\n",
        "\n",
        "        # 4. First passage time data loss\n",
        "        loss_data = self._compute_data_loss(rt1, rt2, xmax)\n",
        "\n",
        "        # Weighted total loss\n",
        "        total_loss = (LAMBDA_PDE * loss_pde +\n",
        "                     LAMBDA_IC * loss_ic +\n",
        "                     LAMBDA_BC * loss_bc +\n",
        "                     LAMBDA_DATA * loss_data)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def _compute_data_loss(self, rt1, rt2, xmax):\n",
        "        \"\"\"Compute loss from first passage time data.\"\"\"\n",
        "        # Upper boundary\n",
        "        tspace1 = np.sort(rt1)\n",
        "        if len(tspace1) > 0:\n",
        "            t1 = tf.constant(tspace1.reshape(-1, 1), 'float32') - tf.math.sigmoid(self.model.tt0) * self.minRT\n",
        "            loss_upper = self._compute_boundary_flux_loss(t1, xmax, tspace1, rt1, rt2, upper=True)\n",
        "        else:\n",
        "            loss_upper = 0.0\n",
        "\n",
        "        # Lower boundary\n",
        "        tspace2 = np.sort(rt2)\n",
        "        if len(tspace2) > 0:\n",
        "            t2 = tf.constant(tspace2.reshape(-1, 1), 'float32') - tf.math.sigmoid(self.model.tt0) * self.minRT\n",
        "            loss_lower = self._compute_boundary_flux_loss(t2, -xmax, tspace2, rt1, rt2, upper=False)\n",
        "        else:\n",
        "            loss_lower = 0.0\n",
        "\n",
        "        return loss_upper + loss_lower\n",
        "\n",
        "    def _compute_boundary_flux_loss(self, t, x_boundary, tspace, rt1, rt2, upper=True):\n",
        "        \"\"\"Compute loss for boundary flux matching.\"\"\"\n",
        "        # Create boundary points\n",
        "        x_points = tf.ones_like(t) * x_boundary\n",
        "        X_boundary = tf.concat([t, x_points], axis=1)\n",
        "\n",
        "        # Compute solution at boundary and nearby points for gradient\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(x_points)\n",
        "            u_boundary = self.model(X_boundary)\n",
        "\n",
        "        # Compute spatial gradient using automatic differentiation\n",
        "        u_x = tape.gradient(u_boundary, x_points)\n",
        "\n",
        "        # Compute flux using Fokker-Planck boundary condition\n",
        "        drift = self.model.lambd0 - self.model.beta * x_boundary\n",
        "        J_pred = drift * u_boundary - 0.5 * (self.model.sig)**2 * u_x\n",
        "\n",
        "        # Empirical distribution from data\n",
        "        kde = gaussian_kde(tspace)\n",
        "        if upper:\n",
        "            weight = len(tspace) / (len(rt1) + len(rt2))\n",
        "            J_empirical = weight * kde(tspace.flatten())\n",
        "        else:\n",
        "            weight = -len(tspace) / (len(rt1) + len(rt2))\n",
        "            J_empirical = weight * kde(tspace.flatten())\n",
        "\n",
        "        J_empirical = tf.constant(J_empirical.reshape(-1, 1), dtype=tf.float32)\n",
        "\n",
        "        return tf.reduce_mean(tf.square(J_pred - J_empirical))\n",
        "\n",
        "    def get_grad(self, X, xmax, rt1, rt2, u_data):\n",
        "        \"\"\"Compute gradients of loss function.\"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.loss_fn(X, xmax, rt1, rt2, u_data)\n",
        "\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        return loss, grads\n",
        "\n",
        "    def solve_with_TFoptimizer(self, optimizer, X, xmax, rt1, rt2, u_data, N=1001):\n",
        "        \"\"\"Solve using TensorFlow optimizer.\"\"\"\n",
        "        @tf.function\n",
        "        def train_step():\n",
        "            loss, grads = self.get_grad(X, xmax, rt1, rt2, u_data)\n",
        "            optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "            return loss\n",
        "\n",
        "        # Training loop\n",
        "        for i in range(N):\n",
        "            loss = train_step()\n",
        "            self.current_loss = loss.numpy()\n",
        "            self._callback()\n",
        "\n",
        "            # Early stopping if loss becomes NaN\n",
        "            if np.isnan(self.current_loss):\n",
        "                print(\"Training stopped due to NaN loss\")\n",
        "                break\n",
        "\n",
        "    def _callback(self):\n",
        "        \"\"\"Callback function for training progress.\"\"\"\n",
        "        # Track parameters\n",
        "        self.model.lambd0_list.append(self.model.lambd0.numpy())\n",
        "        self.model.beta_list.append(self.model.beta.numpy())\n",
        "        self.model.sig_list.append(self.model.sig.numpy())\n",
        "        self.model.tt0_list.append((tf.math.sigmoid(self.model.tt0) * self.minRT).numpy())\n",
        "\n",
        "        if self.iter % 100 == 0:\n",
        "            print(f'Iter {self.iter:05d}: Loss = {self.current_loss:10.8e}')\n",
        "            print(f'         drift0 = {self.model.lambd0.numpy():.4f}, '\n",
        "                  f'beta = {self.model.beta.numpy():.4f}, '\n",
        "                  f'sigma = {self.model.sig.numpy():.4f}, '\n",
        "                  f't0 = {(tf.math.sigmoid(self.model.tt0) * self.minRT).numpy():.4f}')\n",
        "\n",
        "        self.hist.append(self.current_loss)\n",
        "        self.iter += 1\n",
        "\n",
        "    # Parameter getter methods\n",
        "    def get_parameters(self):\n",
        "        \"\"\"Get current estimated parameters.\"\"\"\n",
        "        return {\n",
        "            'drift0': float(self.model.lambd0.numpy()),\n",
        "            'beta': float(self.model.beta.numpy()),\n",
        "            'sigma': float(self.model.sig.numpy()),\n",
        "            't0': float((tf.math.sigmoid(self.model.tt0) * self.minRT).numpy())\n",
        "        }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Utility Functions\n",
        "# =============================================================================\n",
        "\n",
        "def dirac_delta_function(x, delta, x0):\n",
        "    \"\"\"Approximate Dirac delta function as Gaussian distribution.\"\"\"\n",
        "    return 1 / (2 * np.sqrt(np.pi * delta)) * tf.math.exp(-((x - x0)**2) / (4 * delta))\n",
        "\n",
        "\n",
        "def initial_condition(x):\n",
        "    \"\"\"Define initial condition for Fokker-Planck equation.\"\"\"\n",
        "    return dirac_delta_function(x, DELTA, 0.0)\n",
        "\n",
        "\n",
        "def prepare_training_data(lb, ub, n_collocation=100, n_boundary=100, n_initial=50):\n",
        "    \"\"\"Prepare collocation, boundary, and initial condition data.\"\"\"\n",
        "    # Collocation points\n",
        "    tspace = np.linspace(lb[0], ub[0], n_collocation + 1)\n",
        "    xspace = np.linspace(lb[1], ub[1], n_collocation + 1)\n",
        "    T, X = np.meshgrid(tspace, xspace)\n",
        "    Xgrid = np.vstack([T.flatten(), X.flatten()]).T\n",
        "    Xgrid = tf.constant(Xgrid, 'float32')\n",
        "\n",
        "    # Boundary conditions (both boundaries)\n",
        "    t_b1 = tf.random.uniform((n_boundary//2, 1), lb[0], ub[0], dtype='float32')\n",
        "    x_b1 = tf.ones((n_boundary//2, 1), dtype='float32') * ub[1]  # Upper boundary\n",
        "    X_b1 = tf.concat([t_b1, x_b1], axis=1)\n",
        "    u_b1 = tf.zeros((n_boundary//2, 1), 'float32')\n",
        "\n",
        "    t_b2 = tf.random.uniform((n_boundary//2, 1), lb[0], ub[0], dtype='float32')\n",
        "    x_b2 = tf.ones((n_boundary//2, 1), dtype='float32') * lb[1]  # Lower boundary\n",
        "    X_b2 = tf.concat([t_b2, x_b2], axis=1)\n",
        "    u_b2 = tf.zeros((n_boundary//2, 1), 'float32')\n",
        "\n",
        "    X_b = tf.concat([X_b1, X_b2], axis=0)\n",
        "    u_b = tf.concat([u_b1, u_b2], axis=0)\n",
        "\n",
        "    # Initial condition\n",
        "    t_0 = tf.ones((n_initial, 1), dtype='float32') * lb[0]\n",
        "    x_0 = tf.linspace(lb[1], ub[1], n_initial)\n",
        "    x_0 = tf.reshape(x_0, (n_initial, 1))\n",
        "    u_0 = initial_condition(x_0)\n",
        "    X_0 = tf.concat([t_0, x_0], axis=1)\n",
        "\n",
        "    return Xgrid, [X_0, X_b], [u_0, u_b]\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Analysis and Visualization\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_results(solver, true_params, rt1, rt2):\n",
        "    \"\"\"Analyze and display estimation results.\"\"\"\n",
        "    estimated = solver.get_parameters()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PARAMETER ESTIMATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'Parameter':<12} {'True':<10} {'Estimated':<12} {'Error':<10} {'Relative Error':<15}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    for param in ['drift0', 'beta', 'sigma', 't0']:\n",
        "        true_val = true_params[param]\n",
        "        est_val = estimated[param]\n",
        "        error = abs(true_val - est_val)\n",
        "        rel_error = error / abs(true_val) * 100\n",
        "\n",
        "        print(f\"{param:<12} {true_val:<10.4f} {est_val:<12.4f} {error:<10.4f} {rel_error:<15.2f}%\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.semilogy(solver.hist)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss History')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(solver.model.lambd0_list, label='drift0')\n",
        "    plt.plot(solver.model.beta_list, label='beta')\n",
        "    plt.plot(solver.model.sig_list, label='sigma')\n",
        "    plt.axhline(y=true_params['drift0'], color='r', linestyle='--', alpha=0.7)\n",
        "    plt.axhline(y=true_params['beta'], color='g', linestyle='--', alpha=0.7)\n",
        "    plt.axhline(y=true_params['sigma'], color='b', linestyle='--', alpha=0.7)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Parameter Value')\n",
        "    plt.title('Parameter Convergence')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Main Execution\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run OU-DDM parameter estimation.\"\"\"\n",
        "    print(\"TINN Parameter Estimation for Ornstein-Uhlenbeck DDM\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # True parameters\n",
        "    true_params = {\n",
        "        'drift0': DRIFT_0,\n",
        "        'beta': BETA,\n",
        "        'sigma': NOISE_STD,\n",
        "        't0': NON_DECISION_TIME\n",
        "    }\n",
        "\n",
        "    # Generate synthetic data\n",
        "    print(\"Generating synthetic OU-DDM data...\")\n",
        "    decision_times1, decision_times2, _, _ = simulate_ou_ddm_rk4(\n",
        "        NUM_SIMULATIONS, TIME_STEPS, BOUNDARY_SEPARATION, STARTING_POINT,\n",
        "        NOISE_STD, DT, DRIFT_0, BETA)\n",
        "\n",
        "    # Add non-decision time\n",
        "    rt1 = decision_times1 + NON_DECISION_TIME\n",
        "    rt2 = decision_times2 + NON_DECISION_TIME\n",
        "\n",
        "    print(f\"Generated {len(rt1)} upper and {len(rt2)} lower boundary crossings\")\n",
        "    print(f\"Mean RT upper: {np.mean(rt1):.3f}s, lower: {np.mean(rt2):.3f}s\")\n",
        "\n",
        "    # Prepare domain\n",
        "    max_time = max(np.max(rt1) if len(rt1) > 0 else 0,\n",
        "                   np.max(rt2) if len(rt2) > 0 else 0)\n",
        "    lb = tf.constant([0.0, XMIN], dtype='float32')\n",
        "    ub = tf.constant([max_time * 1.1, XMAX], dtype='float32')  # 10% buffer\n",
        "\n",
        "    # Prepare training data\n",
        "    Xgrid, X_data, u_data = prepare_training_data(lb, ub, N_COLLOCATION, N_BOUNDARY, N_INITIAL)\n",
        "\n",
        "    # Initialize model\n",
        "    model = PINNIdentificationNet(lb, ub,\n",
        "                                 num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
        "                                 num_neurons_per_layer=NUM_NEURONS_PER_LAYER,\n",
        "                                 activation='tanh',\n",
        "                                 kernel_initializer='glorot_normal')\n",
        "\n",
        "    # Initialize solver\n",
        "    solver = F_P_PINNIdentification(model, Xgrid)\n",
        "\n",
        "    # Set up optimizer with learning rate schedule\n",
        "    lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "        [1000, 5000, 15000], [1e-2, 5e-3, 1e-3, 5e-4])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\nStarting TINN training...\")\n",
        "    print(\"Initial parameters:\")\n",
        "    print(f\"  drift0 = {model.lambd0.numpy():.4f}, beta = {model.beta.numpy():.4f}, \"\n",
        "          f\"sigma = {model.sig.numpy():.4f}\")\n",
        "\n",
        "    t0 = time()\n",
        "    solver.solve_with_TFoptimizer(optimizer, X_data, XMAX, rt1, rt2, u_data, N=NUM_EPOCHS)\n",
        "    training_time = time() - t0\n",
        "\n",
        "    print(f'\\nTraining completed in {training_time:.2f} seconds')\n",
        "\n",
        "    # Analyze results\n",
        "    analyze_results(solver, true_params, rt1, rt2)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}